{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Multimodal RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to create a multimodal RAG for scientific papers that leverages two models:\n",
    "* Llama4 for it’s vision capability, it is hosted on [Nscale serverless](https://www.nscale.com) \n",
    "* ColPali a VLM model capable of generating accurate embeddings of image data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with RAG here's a quick overview:\n",
    "1. Indexing phase:\n",
    "   1. The user first uploads a document\n",
    "   2. The document's content is then split into chunks\n",
    "   3. Those chunks are fed to an embedding model that will convert the text to a vector of number that captures the sementic meaning of the chunk\n",
    "   4. The chunk is then stored in a vector store or database\n",
    "2. Retrieval phase:\n",
    "   1. The user will query the system, \n",
    "   2. The query itself will be converted to an embedding\n",
    "   3. A vector similarity search between the query vector and the vectors stored in the database then happens. \n",
    "3. Generation phase:\n",
    "   1. Once the vectors are retrieved, we use a large language model to generate a response based on the query and the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put diagram here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple RAGs can work well, they often fall short in tasks that requires parsing complex layouts. For example scientific paper, are complex to parse because of their sometimes complex structure. \n",
    "One solution was to leverage projects such as LlamaParse or unstructured.io to parse those documents using OCR among other techniques. Such approach can work well but lead to overhead time in the indexing phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore building a RAG using ColPali. In high level, ColPali is based of PaliGemma-3B a vision language model that is further enhanced to generate ColBERT-style multi-vector representations of text and image data, among other optimisations. This model can therefore be used for multimodal retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the concept is not clear by now, do not worry as we will be building a quick multimodal RAG system to answer any questions related to ColPali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv # arXiv API\n",
    "!pip install byaldi # RAG model\n",
    "!pip install pdf2image # Convert pdf to images\n",
    "!pip install openai # LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install poppler\n",
    "# !sudo apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "nscale_api_key = os.environ[\"NSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the arXiv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't heard of [arXiv](https://arxiv.org), in brief it's an open-access repository where researchers share preprints of scientific papers before formal peer review, primarily in fields like physics, mathematics, and computer science.\n",
    "\n",
    "For our use case we will be using the [paper](https://arxiv.org/pdf/2407.01449) \"ColPali: EFFICIENT DOCUMENT RETRIEVAL WITH VISION LANGUAGE MODELS\" by Faysse et al.\n",
    "\n",
    "And leverage [arxiv's API](https://info.arxiv.org/help/api/basics.html) to retrieve the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/4x5mb1js11l6skh4_n7rftwc0000gn/T/ipykernel_40516/3546485162.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/2407.01449v6.pdf'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for the most relevant paper on ColPali and download it to our data folder.\n",
    "\n",
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(\n",
    "        query=\"ColPali\",\n",
    "        max_results=1,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "\n",
    "results = list(search.results())\n",
    "paper = results[0]\n",
    "\n",
    "download_dir = \"data\"\n",
    "\n",
    "pdf_path = os.path.join(download_dir, f\"{paper.get_short_id()}.pdf\")\n",
    "paper.download_pdf(filename=pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the multimodal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to initialise ColPali model using the byaldi library. [ColPali](https://huggingface.co/vidore/colpali-v1.2) will be used to generate the embeddings of the document. It does so by converting the document into images that will then be cut into patches, these patches are later embedded in a 128 dimension vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ColPali_retrieval](./images/ColPali_retrieval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 34379.54it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "# Initialise the multimodal model\n",
    "retrieval_model = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the data\n",
    "\n",
    "retrieval_model.index(\n",
    "    input_path=\"data/\", index_name=\"image_index\", store_collection_with_index=True, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the new Llama 4 Scout as LLM, it is a 17 billion active parameter model with 16 experts that uses a mixture-of-experts (MoE) architecture. It's a very powerful multimodal model with native multimodality, strong performance and an extremely large context window.\n",
    "\n",
    "Running such model locally is not feasiable. For this reason we will be inferencing the model through [Nscale serverless](https://www.nscale.com)! Nscale offers 5$ of free credit upon signup, way more than enough to fully understand the ColPali paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initilise the client\n",
    "nscale_base_url = \"https://inference.api.nscale.com/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=nscale_api_key,\n",
    "    base_url=nscale_base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retrieval model on the ColPali paper\n",
    "\n",
    "query = \"Can you explain the Late interaction formula in detail?\"\n",
    "returned_page = retrieval_model.search(query, k=2)[0].base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Late Interaction formula, as presented in the text, is used to calculate the interaction between a query and a document in a Vision-Language Model (VLM). \n",
      "\n",
      "**Late Interaction Formula**\n",
      "\n",
      "The Late Interaction formula is defined as:\n",
      "\n",
      "$\\operatorname{LI}(q, d)=\\sum_{i \\in\\left[1, N_{q}\\right]} \\max _{j \\in\\left[1, N_{d}\\right]}\\left\\langle\\mathbf{E}_{\\mathbf{q}}^{(i)} \\mid \\mathbf{E}_{\\mathbf{d}}^{(j)}\\right\\rangle$\n",
      "\n",
      "where:\n",
      "\n",
      "*   $\\mathbf{E}_{\\mathbf{q}} \\in \\mathbb{R}^{N_{q} \\times D}$ and $\\mathbf{E}_{\\mathbf{d}} \\in \\mathbb{R}^{N_{d} \\times D}$ are vector representations of the query and document, respectively.\n",
      "*   $N_q$ and $N_d$ are the number of vectors in the query and in the document page embeddings.\n",
      "*   $\\langle\\cdot \\mid \\cdot\\rangle$ is the dot product.\n",
      "\n",
      "**Definition of the Variables**\n",
      "\n",
      "*   $q$ is the query.\n",
      "*   $d$ is the document.\n",
      "*   $\\mathbf{E}_{\\mathbf{q}}$ and $\\mathbf{E}_{\\mathbf{d}}$ are the embedding vectors of $q$ and $d$, respectively.\n",
      "*   $N_q$ and $N_d$ are the number of vectors in $q$ and $d$, respectively.\n",
      "*   $\\mathbf{E}_{\\\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": query},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{returned_page}\", \n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
