{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Multimodal RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives as a practical guide to inspire developers and researchers in building AI applications. Through a hands-on example of creating a multimodal RAG (Retrieval Augmented Generation) system for scientific papers, we'll explore:\n",
    "\n",
    "- How to leverage Large Language Models (LLMs) hosted on [Nscale serverless](https://www.nscale.com) platform\n",
    "- The fundamentals of building effective RAG systems\n",
    "\n",
    "While we'll be building a specific implementation for processing arXiv papers, the concepts and patterns demonstrated here can be adapted to create various AI services and applications. \n",
    "\n",
    "Whether you're looking to understand RAG systems, explore multimodal AI, or learn how to utilize cloud-based LLMs effectively, this notebook provides a foundation to build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlines of a simple RAG \n",
    "If you are not familiar with RAG, it is a technique that enhances AI language models by first retrieving relevant information from a knowledge base, then using that context to generate more accurate and informed responses. Think of it as giving an AI model access to a specialized library that it can reference before answering questions. Here's the outline of a simple RAG workflow:\n",
    "\n",
    "1. Indexing phase:\n",
    "   1. The user first uploads a document\n",
    "   2. The document's content is then split into chunks\n",
    "   3. Those chunks are fed to an embedding model that will convert the text to a vector of number that captures the sementic meaning of the chunk\n",
    "   4. The chunk is then stored in a vector store or database\n",
    "2. Retrieval phase:\n",
    "   1. The user will query the system\n",
    "   2. The query itself will be converted to an embedding\n",
    "   3. A vector similarity search between the query vector and the vectors stored in the database then happens. \n",
    "3. Generation phase:\n",
    "   1. Once the vectors are retrieved, we use a large language model to generate a response based on the query and the retrieved context.\n",
    "\n",
    "![basic_RAG_figure](../images/basic_rag_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal RAG\n",
    "While simple RAGs can work well, they often fall short in tasks that requires parsing complex layouts. For example scientific paper, are complex to parse because of their sometimes difficult structure which can include text, image and tabular data. \n",
    "\n",
    "One solution has been to leverage projects such as LlamaParse or unstructured.io to parse those documents using OCR, layout detection and captioning. Such approach can work well but will lead to overhead time in the indexing phase as seen in the following figure from [ColPali: EFFICIENT DOCUMENT RETRIEVAL WITH VISION LANGUAGE MODELS](https://arxiv.org/pdf/2407.01449) by Faysse et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![colpali_parsing_comparaison](../images/offline_document_indexing_colpali.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore in this notebook we will explore an efficient and simpler approach to building a multimodal RAG using the VLM ColPali. In high level, ColPali is based of PaliGemma-3B a vision language model that is further enhanced to generate ColBERT-style multi-vector representations of text and image data, among other optimisations. The model directly encode pages and can be used for multimodal retrieval tasks.\n",
    "\n",
    "To achieve the multimodal RAG system for scientific papers we are going to combine two powerful models:\n",
    "* Llama4 for itâ€™s vision capability, it is hosted on [Nscale serverless](https://www.nscale.com) \n",
    "* ColPali a VLM model capable of generating accurate embeddings of image data. \n",
    "\n",
    "If the concept is not clear by now, do not worry as we will be building a simple multimodal RAG system to answer any questions related to ColPali.\n",
    "\n",
    "Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv # arXiv API\n",
    "!pip install byaldi # RAG model\n",
    "!pip install pdf2image # Convert pdf to images\n",
    "!pip install openai # LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install poppler\n",
    "# !sudo apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nscale_api_key = \"xxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the arXiv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't heard of [arXiv](https://arxiv.org), in brief it's an open-access repository where researchers share preprints of scientific papers before formal peer review, primarily in fields like physics, mathematics, and computer science.\n",
    "\n",
    "For our use case we will be using the [paper](https://arxiv.org/pdf/2407.01449) \"ColPali: EFFICIENT DOCUMENT RETRIEVAL WITH VISION LANGUAGE MODELS\" by Faysse et al.\n",
    "\n",
    "And leverage [arxiv's API](https://info.arxiv.org/help/api/basics.html) to retrieve the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the most relevant paper on ColPali and download it to our data folder.\n",
    "\n",
    "import arxiv\n",
    "import os\n",
    "\n",
    "search = arxiv.Search(\n",
    "        query=\"ColPali\",\n",
    "        max_results=1,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "\n",
    "results = list(search.results())\n",
    "paper = results[0]\n",
    "\n",
    "download_dir = \"data\"\n",
    "\n",
    "pdf_path = os.path.join(download_dir, f\"{paper.get_short_id()}.pdf\")\n",
    "paper.download_pdf(filename=pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the multimodal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to initialise ColPali model using the byaldi library. [ColPali](https://huggingface.co/vidore/colpali-v1.2) will be used to generate the embeddings of the document. It does so by converting the document into images that will then be cut into patches, these patches are later embedded in a 128 dimension vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ColPali_retrieval](../images/ColPali_retrieval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ColPali: EFFICIENT DOCUMENT RETRIEVAL WITH VISION LANGUAGE MODELS](https://arxiv.org/pdf/2407.01449) by Faysse et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "# Initialise the multimodal model\n",
    "retrieval_model = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the data\n",
    "\n",
    "retrieval_model.index(\n",
    "    input_path=\"data/\", index_name=\"image_index\", store_collection_with_index=True, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise LLM\n",
    "We will use the new Llama 4 Scout as LLM, it is a 17 billion active parameter model with 16 experts that uses a mixture-of-experts (MoE) architecture. It's a very powerful multimodal model with native multimodality, strong performance and an extremely large context window.\n",
    "\n",
    "Running such model locally is not feasiable. For this reason we will be inferencing the model through [Nscale serverless](https://www.nscale.com)! Nscale offers 5$ of free credit upon signup, way more than enough to fully understand the ColPali paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval and generation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initilise the client\n",
    "nscale_base_url = \"https://inference.api.nscale.com/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=nscale_api_key,\n",
    "    base_url=nscale_base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retrieval model on the ColPali paper\n",
    "\n",
    "query = \"Describe the results of table 2\"\n",
    "returned_page = retrieval_model.search(query, k=2)[0].base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table presents results that compare various models and their performance across multiple metrics. \n",
      "\n",
      "## Step 1: Identify the metrics and models presented\n",
      "The table provides a comprehensive evaluation of baseline models and proposed methods on ViDoRe, with only visual elements and text-only metrics not computed for benchmarks.\n",
      "\n",
      "## Step 2: Analyze the performance of different models\n",
      "The models are evaluated based on various metrics such as ArxivQ, DocQ, InfoQ, TabF, TATQ, Shift, AI, Energy, Gov, Health, and Avg.\n",
      "\n",
      "## 3: Compare results across different models and methods\n",
      "Results are presented using Recall@1 metrics.\n",
      "\n",
      "## 4: Conclusion\n",
      "Based on the provided information, the description of table 2 results could not be found, however, table 6 and 7 results are provided.\n",
      "\n",
      "## 5: Results from Table 6 and Table 7\n",
      "The best result in table 6 is from ColPali (+Lac Iter.) which scores 72.7 and has a recall of 85.0. \n",
      "\n",
      "The best results in table 7 are from ColQwen2 (224) which scores 86.6 and 86.6 on Energy and Health. \n",
      "\n",
      "## 6: Final Conclusion\n",
      "The description of table 2 results could not be found. However, based on the results from table 6 and 7, ColPali and ColQwen2 models tend to perform better across various metrics.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": query},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{returned_page}\", \n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully built a multimodal RAG system for scientific papers that combines:\n",
    "- ColPali's powerful document retrieval and embedding generation\n",
    "- Llama 4 Scout's advanced vision-language understanding, accessed through Nscale's serverless platform\n",
    "\n",
    "We demonstrated how to:\n",
    "1. Download and process arXiv papers\n",
    "2. Index documents using ColPali's multimodal capabilities\n",
    "3. Perform intelligent retrieval based on user queries\n",
    "4. Generate contextual responses using Llama 4 Scout\n",
    "\n",
    "### What We've Learned\n",
    "- How to implement a multimodal RAG system without complex OCR pipelines\n",
    "- Ways to leverage Nscale's LLMs effectively\n",
    "- Techniques for handling both text and visual content in academic papers\n",
    "\n",
    "### Build Your Own\n",
    "This implementation serves as a starting point - here are some ways you could extend it:\n",
    "- Adapt the system for other document types (patents, technical documentation, etc.)\n",
    "- Add more sophisticated indexing, retrieval and generation strategies\n",
    "- Implement concurrent processing for large document collections\n",
    "- Create a web interface or API\n",
    "- Fine-tune the models for your specific use case\n",
    "\n",
    "Remember, the patterns shown here can be adapted for various use cases - from document analysis to building complete AI services. Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
